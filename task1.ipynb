{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(num_samples=10000, seq_length=5):\n",
    "    \"\"\"\n",
    "    Generate a dataset consisting of two types of sequences:\n",
    "    - Reversed sequences (labeled as 1)\n",
    "    - Shuffled sequences (labeled as 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    x_data_part1 = np.random.randint(1, 10, (num_samples // 2, seq_length))  # Generate random integer sequences\n",
    "    x_data_part2 = np.random.randint(1, 10, (num_samples // 2, seq_length))  \n",
    "\n",
    "    x_data_part1 = np.concatenate([x_data_part1, np.flip(x_data_part1, axis=1).copy()], axis=1)  # Concatenate with reversed version\n",
    "    y_data_part1 = np.ones(num_samples // 2)  # Label reversed sequences as 1\n",
    "\n",
    "    shuffled_x = np.array([np.random.permutation(row) for row in x_data_part2])\n",
    "    x_data_part2 = np.concatenate([x_data_part2, shuffled_x], axis=1)  # Concatenate with shuffled version\n",
    "    y_data_part2 = np.zeros(num_samples // 2)  # Label shuffled sequences as 0\n",
    "\n",
    "    # Combine the two parts\n",
    "    x_data = np.concatenate([x_data_part1, x_data_part2], axis=0)\n",
    "    y_data = np.concatenate([y_data_part1, y_data_part2], axis=0)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    shuffle_indices = np.random.permutation(len(x_data))\n",
    "    x_data = x_data[shuffle_indices]\n",
    "    y_data = y_data[shuffle_indices]\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "# Generate dataset\n",
    "x_data, y_data = generate_data(num_samples=10000)\n",
    "\n",
    "# Split data into train (80%) and test (20%) sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_log_reg = log_reg.predict(x_test)\n",
    "\n",
    "# Compute accuracy for logistic regression\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(accuracy_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6476, Learning Rate: 0.050000\n",
      "Epoch 5, Loss: 0.1445, Learning Rate: 0.049692\n",
      "Epoch 10, Loss: 0.1120, Learning Rate: 0.048776\n",
      "Epoch 15, Loss: 0.0964, Learning Rate: 0.047275\n",
      "Epoch 20, Loss: 0.0886, Learning Rate: 0.045225\n",
      "Epoch 25, Loss: 0.0827, Learning Rate: 0.042678\n",
      "Epoch 30, Loss: 0.0752, Learning Rate: 0.039695\n",
      "Epoch 35, Loss: 0.0624, Learning Rate: 0.036350\n",
      "Epoch 40, Loss: 0.0618, Learning Rate: 0.032725\n",
      "Epoch 45, Loss: 0.0599, Learning Rate: 0.028911\n",
      "Epoch 50, Loss: 0.0578, Learning Rate: 0.025000\n",
      "Epoch 55, Loss: 0.0545, Learning Rate: 0.021089\n",
      "Epoch 60, Loss: 0.0540, Learning Rate: 0.017275\n",
      "Epoch 65, Loss: 0.0530, Learning Rate: 0.013650\n",
      "Epoch 70, Loss: 0.0529, Learning Rate: 0.010305\n",
      "Epoch 75, Loss: 0.0524, Learning Rate: 0.007322\n",
      "Epoch 80, Loss: 0.0520, Learning Rate: 0.004775\n",
      "Epoch 85, Loss: 0.0518, Learning Rate: 0.002725\n",
      "Epoch 90, Loss: 0.0515, Learning Rate: 0.001224\n",
      "Epoch 95, Loss: 0.0514, Learning Rate: 0.000308\n",
      "Neural Network Accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-10  # Avoid log(0)\n",
    "    return -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Neural network class with training and prediction\n",
    "class SimpleNN:\n",
    "    def __init__(self, input_size, hidden_size=20, learning_rate=0.01, epochs=100):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.start_learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, 1) * 0.1\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = sigmoid(self.Z1)\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        return self.A2\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backward propagation and gradient update\"\"\"\n",
    "        num_samples = X.shape[0]\n",
    "\n",
    "        # Compute error\n",
    "        dZ2 = self.A2 - y.reshape(-1, 1)\n",
    "        dW2 = np.dot(self.A1.T, dZ2) / num_samples\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * sigmoid_derivative(self.Z1)\n",
    "        dW1 = np.dot(X.T, dZ1) / num_samples\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def train(self, X_train, y_train, batch_size=4):\n",
    "        \"\"\"Train the neural network using mini-batch gradient descent\"\"\"\n",
    "        num_samples = X_train.shape[0]\n",
    "        num_batches = num_samples // batch_size\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # cosine decrease\n",
    "            self.learning_rate = 0.5 * (self.start_learning_rate) * (1 + np.cos(epoch / self.epochs * np.pi))\n",
    "\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            epoch_loss = 0 \n",
    "            \n",
    "            for i in range(num_batches):\n",
    "\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "                X_batch = X_train[start:end]\n",
    "                y_batch = y_train[start:end]\n",
    "\n",
    "                y_pred = self.forward(X_batch)\n",
    "\n",
    "                loss = binary_cross_entropy(y_batch, y_pred)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                self.backward(X_batch, y_batch)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                avg_loss = epoch_loss / num_batches\n",
    "                losses.append(avg_loss)\n",
    "                print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Learning Rate: {self.learning_rate:.6f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize and train the neural network\n",
    "    nn = SimpleNN(input_size=10, hidden_size=10, learning_rate=0.05, epochs=100)\n",
    "    nn.train(x_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_nn = nn.predict(x_test)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "    print(f\"Neural Network Accuracy: {accuracy_nn:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
